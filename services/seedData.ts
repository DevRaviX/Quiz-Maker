export const initialSeedData = {
  bank: {
    name: "Data Science MCQ Sample Paper",
    description: "A comprehensive sample paper covering Data Analysis, Machine Learning, Data Visualization, and more.",
  },
  questions: [
    // Unit 3: Data Analysis and Machine Learning
    // Section A: Descriptive Statistics and Probability
    {
      text: "Which of the following is **NOT** a measure of central tendency?",
      options: ["Mean", "Median", "Mode", "Variance"],
      correctAnswerIndex: 3,
      explanation: "Measures of central tendency (Mean, Median, Mode) describe the typical or central value of a dataset. Variance is a measure of **dispersion** (or spread) in the data.",
    },
    {
      text: "The value that occurs most frequently in a data set is called the:",
      options: ["Mean", "Median", "Mode", "Range"],
      correctAnswerIndex: 2,
      explanation: "The Mode is defined as the value with the highest frequency in a data set.",
    },
    {
      text: "The median is the:",
      options: ["Average of all values", "Middle value when the data is ordered", "Most frequent value", "Square root of the variance"],
      correctAnswerIndex: 1,
      explanation: "The Median is the value separating the higher half from the lower half of a data sample, which requires the data to be sorted first.",
    },
    {
      text: "In a perfectly symmetrical distribution, which of the following is true?",
      options: ["Mean > Median > Mode", "Mean < Median < Mode", "Mean = Median = Mode", "Mean and Median are equal, but Mode is different"],
      correctAnswerIndex: 2,
      explanation: "For a normal (symmetrical) distribution, the central tendency measures coincide at the center of the distribution.",
    },
    {
      text: 'What does the term "Distribution" primarily refer to in statistics?',
      options: ["The physical location of the data points", "The way data is spread out or grouped", "The average value of the data", "The sample size"],
      correctAnswerIndex: 1,
      explanation: "A distribution shows the possible values for a variable and how often they occur, illustrating the pattern of spread and grouping of the data.",
    },
    {
      text: "The standard deviation is the square root of the:",
      options: ["Mean", "Median", "Variance", "Skewness"],
      correctAnswerIndex: 2,
      explanation: "Standard deviation is calculated by taking the square root of the variance, providing a measure of spread in the original units of the data.",
    },
    {
      text: "A large variance indicates that the data points are:",
      options: ["Close to the mean", "Far from the mean", "Always positive", "Only two distinct values"],
      correctAnswerIndex: 1,
      explanation: "Variance is the average of the squared differences from the Mean; a larger value means the data points are more spread out from the average.",
    },
    {
      text: "The Central Limit Theorem (CLT) states that the distribution of sample means approaches a normal distribution as the sample size:",
      options: ["Decreases", "Increases", "Remains constant", "Approaches zero"],
      correctAnswerIndex: 1,
      explanation: "The CLT is a fundamental theorem stating that for a large enough sample size (typically $n \\ge 30$), the distribution of the sample means will be approximately normal, regardless of the population's distribution.",
    },
    {
      text: "A sample is a subset of the:",
      options: ["Population", "Variance", "Distribution", "Algorithm"],
      correctAnswerIndex: 0,
      explanation: "A population is the entire group you want to draw conclusions about, and a sample is the smaller, representative group actually studied.",
    },
    {
      text: "The sum of the deviations of the data points from the mean is always:",
      options: ["Positive", "Negative", "Zero", "Depends on the data"],
      correctAnswerIndex: 2,
      explanation: "By definition of the mean, the positive and negative deviations from the mean cancel each other out, resulting in a sum of zero.",
    },
    {
      text: "The term used to describe the degree of asymmetry of a distribution around its mean is:",
      options: ["Kurtosis", "Variance", "Skewness", "Standard Error"],
      correctAnswerIndex: 2,
      explanation: 'Skewness measures the distortion of a symmetrical distribution or asymmetry in a set of data. Kurtosis measures the "tailedness" (peakedness).',
    },
    {
      text: "If the data is ordered, the $75^{th}$ percentile is also known as the:",
      options: ["First Quartile", "Second Quartile", "Third Quartile", "Interquartile Range"],
      correctAnswerIndex: 2,
      explanation: "The quartiles divide the ordered data into four equal parts: $25^{th}$ percentile ($Q1$), $50^{th}$ percentile ($Q2$ or Median), and $75^{th}$ percentile ($Q3$).",
    },
    {
      text: "Which measure of central tendency is most affected by outliers?",
      options: ["Median", "Mode", "Mean", "Interquartile Range"],
      correctAnswerIndex: 2,
      explanation: "The Mean (average) uses the value of every data point in its calculation, so extreme values (outliers) will pull the mean significantly in their direction.",
    },
    {
      text: "The Interquartile Range (IQR) is calculated as:",
      options: ["Q3 - Q1", "Maximum - Minimum", "Q2 - Q1", "Standard Deviation / Mean"],
      correctAnswerIndex: 0,
      explanation: "The IQR is a measure of statistical dispersion, calculated as the difference between the third quartile ($Q3$) and the first quartile ($Q1$).",
    },
    {
      text: "Which distribution is most appropriate for modeling count data over a fixed interval (e.g., number of website clicks per minute)?",
      options: ["Normal distribution", "Binomial distribution", "Exponential distribution", "Poisson distribution"],
      correctAnswerIndex: 3,
      explanation: "The Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant rate.",
    },
    // Section B: Data Preprocessing and Modeling Concepts
    {
      text: "What is the process of cleaning, transforming, and consolidating data called?",
      options: ["Data Modeling", "Data Visualization", "Data Preprocessing/Wrangling", "Data Mining"],
      correctAnswerIndex: 2,
      explanation: "Data preprocessing, often called data wrangling, involves steps like cleaning, transforming, and integrating raw data to make it suitable for analysis or machine learning.",
    },
    {
      text: "The term for identifying and handling missing values in a dataset is:",
      options: ["Outlier detection", "Imputation", "Feature Scaling", "Binning"],
      correctAnswerIndex: 1,
      explanation: "Imputation is the process of replacing missing data with substituted values, such as the mean, median, or mode.",
    },
    {
      text: "In Data Science, a **Feature** is also commonly referred to as an:",
      options: ["Output", "Instance", "Attribute or Predictor", "Cluster"],
      correctAnswerIndex: 2,
      explanation: "A feature is an individual measurable property or characteristic of a phenomenon being observed; it is synonymous with attribute or predictor variable.",
    },
    {
      text: "What is the process of reducing the number of variables (features) in a dataset called?",
      options: ["Feature Engineering", "Feature Selection/Dimensionality Reduction", "Feature Expansion", "Feature Imputation"],
      correctAnswerIndex: 1,
      explanation: "Dimensionality reduction aims to reduce the number of random variables under consideration, often by obtaining a set of principal variables.",
    },
    {
      text: "In statistical terminology, an **Outlier** is a data point that:",
      options: ["Has a value of zero", "Is significantly distant from other observations", "Is a missing value", "Is the mean of the dataset"],
      correctAnswerIndex: 1,
      explanation: "An outlier is an observation point that is distant from other observations, often indicating variability in a measurement or an experimental error.",
    },
    {
      text: "Which type of machine learning involves training a model on data with labeled examples (input-output pairs)?",
      options: ["Unsupervised Learning", "Reinforcement Learning", "Supervised Learning", "Deep Learning"],
      correctAnswerIndex: 2,
      explanation: "Supervised learning requires a training dataset where the desired output (label) is known for each input, allowing the model to learn the mapping function.",
    },
    {
      text: "Overfitting in a machine learning model means the model performs:",
      options: ["Poorly on both training and test data", "Well on training data but poorly on unseen test data", "Well on both training and test data", "Randomly on all data"],
      correctAnswerIndex: 1,
      explanation: "Overfitting occurs when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new, unseen data (generalization).",
    },
    {
      text: "The measure of how different the predictions are from the actual values is known as the:",
      options: ["Accuracy", "Error or Residual", "Precision", "Recall"],
      correctAnswerIndex: 1,
      explanation: "In a statistical or machine learning model, a residual is the difference between the observed value and the estimated value, representing the error of the prediction.",
    },
    {
      text: "The term **Cross-Validation** in machine learning is used to:",
      options: ["Clean the data", "Evaluate the model's performance robustly on unseen data", "Select the features", "Visualize the results"],
      correctAnswerIndex: 1,
      explanation: "Cross-validation is a technique to assess how the results of a statistical analysis will generalize to an independent dataset, often by partitioning the data into subsets.",
    },
    {
      text: "The process of converting categorical features into numerical format for use in algorithms is called:",
      options: ["Normalization", "Standardization", "Encoding", "Imputation"],
      correctAnswerIndex: 2,
      explanation: "Encoding, such as one-hot encoding or label encoding, is necessary because most machine learning algorithms only work with numerical inputs.",
    },
    {
      text: "The R-squared value in regression indicates:",
      options: ["The correlation coefficient", "The proportion of variance explained by the model", "The prediction error", "The slope of the regression line"],
      correctAnswerIndex: 1,
      explanation: "$R^2$ (Coefficient of Determination) measures the goodness of fit of a model, specifically indicating the fraction of the variance of the dependent variable that is predictable from the independent variables.",
    },
    {
      text: "Which metric is most appropriate for evaluating regression models?",
      options: ["Accuracy", "Precision", "Mean Squared Error (MSE)", "F1-score"],
      correctAnswerIndex: 2,
      explanation: "MSE (or RMSE, MAE) is used for regression tasks to quantify the average squared difference between the predicted and actual continuous values. Accuracy, Precision, and F1-score are for classification.",
    },
    {
      text: "**Regularization** in machine learning helps to:",
      options: ["Increase model complexity", "Prevent overfitting by penalizing large coefficients", "Speed up training", "Remove outliers"],
      correctAnswerIndex: 1,
      explanation: "Regularization techniques (like Lasso or Ridge) add a penalty term to the model's loss function, which discourages overly complex models and reduces overfitting.",
    },
    {
      text: "The bias-variance tradeoff refers to:",
      options: ["Balancing model complexity with generalization", "Choosing between different algorithms", "Selecting features", "Scaling data"],
      correctAnswerIndex: 0,
      explanation: "It describes the conflict between a model's complexity (which can reduce bias but increase variance) and its ability to generalize to new data.",
    },
    {
      text: "The Central Limit Theorem (CLT) is foundational for:",
      options: ["Descriptive Statistics", "Inferential Statistics", "Data Visualization", "Unsupervised Learning"],
      correctAnswerIndex: 1,
      explanation: "The CLT is critical for inferential statistics because it allows us to assume a normal distribution for sample means, which is essential for hypothesis testing and constructing confidence intervals.",
    },
    // Section C: Core Machine Learning Algorithms
    {
      text: "Linear Regression is primarily used for which type of machine learning task?",
      options: ["Classification", "Clustering", "Regression", "Dimensionality Reduction"],
      correctAnswerIndex: 2,
      explanation: "Linear Regression predicts a continuous numerical output (a regression task) based on one or more input features.",
    },
    {
      text: "The goal of Linear Regression is to find the best-fitting line that minimizes the:",
      options: ["Total number of errors", "Sum of squared residuals (errors)", "Mean of the input features", "Product of the features"],
      correctAnswerIndex: 1,
      explanation: "The standard method, Ordinary Least Squares (OLS), aims to minimize the squared difference between the predicted and actual output values (the residuals).",
    },
    {
      text: "Support Vector Machine (SVM) is a supervised learning model used for:",
      options: ["Only Regression", "Only Clustering", "Classification and Regression", "Time Series Forecasting"],
      correctAnswerIndex: 2,
      explanation: "While most famous for classification, there are extensions like Support Vector Regression (SVR) that adapt the model for regression tasks.",
    },
    {
      text: "The key concept in SVM is finding the **hyperplane** that maximizes the **margin** between the:",
      options: ["Features", "Data points", "Classes", "Predictions"],
      correctAnswerIndex: 2,
      explanation: "The optimal hyperplane in SVM is the one that separates the data points of different classes with the largest possible margin.",
    },
    {
      text: 'In the context of SVM, the term "**kernel trick**" is used to:',
      options: ["Reduce the number of features", "Speed up training time", "Transform non-linearly separable data into a higher-dimensional, linearly separable space", "Regularize the model"],
      correctAnswerIndex: 2,
      explanation: "The kernel trick allows SVM to operate in a high-dimensional, implicit feature space without ever explicitly calculating the coordinates of the data in that space, making non-linear separation possible.",
    },
    {
      text: "Which machine learning algorithm is based on Bayes' Theorem with the assumption of independence among predictors?",
      options: ["Linear Regression", "Support Vector Machine (SVM)", "Naive Bayes", "K-Nearest Neighbors (KNN)"],
      correctAnswerIndex: 2,
      explanation: "Naive Bayes is a collection of classification algorithms based on Bayes' Theorem. The \"Naive\" part refers to the strong (and often false) assumption of conditional independence between every pair of features.",
    },
    {
      text: "Naive Bayes is particularly popular for:",
      options: ["Time Series Analysis", "Image Recognition", "Text classification (e.g., spam detection)", "Unsupervised learning"],
      correctAnswerIndex: 2,
      explanation: "Due to its simplicity, speed, and good performance with high-dimensional, sparse data (like text features), Naive Bayes is a baseline for text classification tasks.",
    },
    {
      text: "The 'Naive' part of Naive Bayes refers to the assumption of:",
      options: ["The data being normally distributed", "The features being conditionally independent given the class", "The model being very simple", "The absence of outliers"],
      correctAnswerIndex: 1,
      explanation: "The \"naïve\" assumption is that the presence of a particular feature in a class is independent of the presence of any other feature, given the class variable.",
    },
    {
      text: "In the equation $y \\= B0 \\+ B1 x$, $B0$ represents the:",
      options: ["Slope", "Intercept", "Error term", "Variance"],
      correctAnswerIndex: 1,
      explanation: "In a linear equation, $B0$ is the y-intercept, representing the value of $y$ when $x$ is zero. $B1$ is the slope.",
    },
    {
      text: "In SVM, **support vectors** are:",
      options: ["All data points in the dataset", "Data points that lie on the margin boundaries", "Outliers in the data", "The centroids of clusters"],
      correctAnswerIndex: 1,
      explanation: "Support vectors are the data points closest to the hyperplane (the decision boundary), and they are the only points that directly influence the position and orientation of the hyperplane.",
    },
    {
      text: "Which of the following is considered a **parametric** machine learning algorithm?",
      options: ["K-Nearest Neighbors", "Decision Tree", "Linear Regression", "Random Forest"],
      correctAnswerIndex: 2,
      explanation: "Parametric algorithms (like Linear Regression) simplify the mapping function to a known form (e.g., a line or plane) and learn a fixed number of parameters ($B0$, $B1$, etc.) from the data. Non-parametric models (like KNN) do not assume a fixed functional form.",
    },
    {
      text: "Multicollinearity in regression refers to:",
      options: ["Multiple dependent variables", "High correlation among independent variables", "Non-linear relationships", "Outliers in the data"],
      correctAnswerIndex: 1,
      explanation: "Multicollinearity is a phenomenon in which two or more predictor variables in a multiple regression model are highly correlated with each other, which can make model coefficient estimates unstable.",
    },
    {
      text: "Which is an unsupervised learning algorithm primarily used for clustering?",
      options: ["Linear Regression", "Naive Bayes", "Support Vector Machine", "K-Means"],
      correctAnswerIndex: 3,
      explanation: "K-Means is a classic unsupervised algorithm used to partition $n$ observations into $k$ clusters, where each observation belongs to the cluster with the nearest mean (centroid).",
    },
    {
      text: "The metric $\\frac{True Positives}{True Positives \\+ False Positives}$ represents which of the following classification metrics?",
      options: ["Accuracy", "Recall", "Precision", "F1-Score"],
      correctAnswerIndex: 2,
      explanation: 'Precision answers: "Of all the cases we predicted positive, how many were actually positive?" (TP / (TP + FP)). Recall (Sensitivity) is TP / (TP + FN).',
    },
    {
      text: "**Principal Component Analysis (PCA)** is a widely used data analysis technique primarily for:",
      options: ["Classification", "Regression", "Dimensionality Reduction", "Data Visualization"],
      correctAnswerIndex: 2,
      explanation: "PCA is used to reduce the dimensionality of a dataset by finding a new set of orthogonal axes (Principal Components) that capture the maximum variance in the data.",
    },
    // Unit 4: Data Visualization
    // Section A: Principles and Data Types
    {
      text: "What is the primary purpose of Data Visualization?",
      options: ["To store data efficiently", "To communicate data insights clearly and effectively", "To run machine learning algorithms faster", "To preprocess and clean the data"],
      correctAnswerIndex: 1,
      explanation: "Visualization converts data into a graphic format to help the human brain understand complex patterns, trends, and outliers more easily than looking at raw numbers.",
    },
    {
      text: "Which type of data is represented by values that fall into distinct categories and have no inherent order (e.g., colors)?",
      options: ["Ordinal Data", "Interval Data", "Nominal Data", "Ratio Data"],
      correctAnswerIndex: 2,
      explanation: 'Nominal data are categories that cannot be ranked or ordered (e.g., "Gender" or "City"). Ordinal data *can* be ordered (e.g., "Small, Medium, Large").',
    },
    {
      text: "Data Encodings refer to the process of:",
      options: ["Encrypting the data for security", "Transforming data values into visual properties/marks", "Converting visual properties into data values", "Cleaning the dataset"],
      correctAnswerIndex: 1,
      explanation: "Data encoding is the process of mapping data attributes (e.g., a numerical value or a category name) to visual channels (e.g., position, color, size, shape) of a mark (e.g., a bar, a point).",
    },
    {
      text: "The visual properties of a mark (like a point or a bar) that we use to encode data are called:",
      options: ["Data Types", "Retinal Variables", "Axioms", "Transformations"],
      correctAnswerIndex: 1,
      explanation: '"Retinal Variables" (a term coined by Jacques Bertin) are pre-attentive attributes like color, size, shape, and texture that the brain processes automatically before conscious attention.',
    },
    {
      text: "Which of the following is an example of a **Retinal Variable** (pre-attentive attribute)?",
      options: ["The title of the chart", "Position on the x/y axis", "Color Hue and Size", "Data table legend"],
      correctAnswerIndex: 2,
      explanation: 'Color Hue and Size are attributes that pop out and can be processed pre-attentively by the viewer. Position, while a visual property, is generally considered the *most* accurate visual encoding channel and is not strictly grouped with the other "retinal" variables.',
    },
    {
      text: "In visualization, mapping a continuous numerical variable to the **Color Hue** is generally discouraged because:",
      options: ["The human eye perceives continuous changes in hue poorly for quantitative reading", "Color hue is only for categorical data", "It requires too much computational power", "It makes the chart look unprofessional"],
      correctAnswerIndex: 0,
      explanation: "Color hue is perceived as nominal (categorical), making it difficult for the eye to accurately discern fine, continuous differences in numerical values. Color saturation/lightness is better for continuous data.",
    },
    {
      text: "Data that can be ranked or ordered, but the difference between values is not meaningful (e.g., 'Small', 'Medium', 'Large'), is:",
      options: ["Nominal", "Ordinal", "Interval", "Ratio"],
      correctAnswerIndex: 1,
      explanation: "Ordinal data has a natural ordering, but the distance between the levels is not quantifiable (e.g., a customer satisfaction rating of 1 to 5).",
    },
    {
      text: "The visual encoding channel considered most effective for accurate quantitative comparison is:",
      options: ["Color Hue", "Area", "Position on a common scale", "Shape"],
      correctAnswerIndex: 2,
      explanation: "Studies (like those by Cleveland and McGill) show that comparing the position of marks along a common baseline (like bars in a bar chart) is the most accurate way for a viewer to judge quantitative differences.",
    },
    {
      text: "Data that has a meaningful zero point (e.g., height, weight) is called:",
      options: ["Nominal data", "Ordinal data", "Interval data", "Ratio data"],
      correctAnswerIndex: 3,
      explanation: "Ratio data is the most informative level of measurement, possessing all properties of nominal, ordinal, and interval scales, and also having a true zero point (e.g., zero weight means no weight).",
    },
    {
      text: "Which concept dictates that the visual design should accurately represent the data, avoiding distortion?",
      options: ["Data Encodings", "Visual Hierarchy", "Lie Factor (Tufte)", "Color Theory"],
      correctAnswerIndex: 2,
      explanation: "Edward Tufte's Lie Factor is the ratio of the size of the effect shown in the graphic to the size of the effect in the data, with a factor of 1.0 representing an accurate graphic.",
    },
    {
      text: "The visual encoding of **Orientation** (e.g., angle of lines) is generally best suited for encoding:",
      options: ["Quantitative data", "Categorical data", "Time series data", "Spatial data"],
      correctAnswerIndex: 1,
      explanation: "Orientation is effective for distinguishing categories but poor for encoding fine-grained quantitative differences because the human eye is not very good at judging angles precisely.",
    },
    {
      text: "**Retinal variables** were introduced by:",
      options: ["Cleveland", "Bertin", "Fisher", "Tukey"],
      correctAnswerIndex: 1,
      explanation: "The term and concept were formalized by French cartographer and theorist Jacques Bertin in his 1967 book, *Sémiologie Graphique*.",
    },
    {
      text: "**Position encoding** is generally considered:",
      options: ["Least accurate", "Most accurate", "Unreliable", "Not used in modern tools"],
      correctAnswerIndex: 1,
      explanation: "As per visualization research (e.g., by Cleveland and McGill), positional encoding (like on a scatter plot or bar chart) is the most accurately perceived visual variable.",
    },
    {
      text: "Which concept involves dividing the visual display space to represent the structure of a hierarchical dataset?",
      options: ["Scatter Plot", "Line Chart", "Treemap", "Box Plot"],
      correctAnswerIndex: 2,
      explanation: "A Treemap displays hierarchical data as a set of nested rectangles, where the size of each rectangle is proportional to the value it represents.",
    },
    {
      text: "Which is NOT a standard type of data visualization?",
      options: ["Bar chart", "Heatmap", "Scatter plot", "Compiler graph"],
      correctAnswerIndex: 3,
      explanation: "Bar charts, heatmaps, and scatter plots are standard, fundamental chart types. A compiler graph is a concept from computer science related to code structure, not data visualization.",
    },
    // Section B: Chart Types and Use Cases
    {
      text: "A Bar Chart is best suited for visualizing:",
      options: ["The relationship between two continuous variables", "The distribution of a single continuous variable", "The comparison of quantities across different categories", "Time series data"],
      correctAnswerIndex: 2,
      explanation: "Bar charts use the length of rectangular bars to compare discrete, categorized data (e.g., sales by region, votes by candidate).",
    },
    {
      text: "A Scatter Plot is typically used to show:",
      options: ["Proportions of a whole", "Trends over time", "The relationship or correlation between two numerical variables", "The frequency of categorical data"],
      correctAnswerIndex: 2,
      explanation: "Scatter plots display values for two different numerical variables, where the pattern of the resulting points indicates correlation (e.g., positive, negative, or none).",
    },
    {
      text: "Which type of chart is ideal for visualizing the distribution of a continuous variable?",
      options: ["Pie Chart", "Scatter Plot", "Histogram", "Bar Chart"],
      correctAnswerIndex: 2,
      explanation: "A histogram groups a continuous variable into bins and plots the frequency (or count) of observations falling into each bin, showing the shape of the data's distribution.",
    },
    {
      text: "If you want to show a trend of stock prices over the last year, the most appropriate visualization type is a:",
      options: ["Histogram", "Line Chart", "Stacked Bar Chart", "Heatmap"],
      correctAnswerIndex: 1,
      explanation: "Line charts are excellent for showing data changes over a continuous interval (like time), making trends immediately visible.",
    },
    {
      text: "When visualizing a part-to-whole relationship, the most common type of chart is the:",
      options: ["Line Chart", "Bubble Chart", "Pie Chart or Stacked Bar Chart", "Box Plot"],
      correctAnswerIndex: 2,
      explanation: "Both pie charts and stacked bar charts visually represent how a total is divided into various categories (parts).",
    },
    {
      text: "A Choropleth Map typically uses which visual encoding to represent data values across geographical regions?",
      options: ["Shape", "Size of circles", "Color Saturation/Lightness", "Orientation"],
      correctAnswerIndex: 2,
      explanation: "Choropleth maps use color intensity (saturation or lightness) to represent data values (e.g., population density, income) across predefined geographical areas.",
    },
    {
      text: "Which visualization technique is used to display relationships in network-based data, such as social connections?",
      options: ["Treemap", "Network Graph/Force-directed Layout", "Area Chart", "Parallel Coordinates Plot"],
      correctAnswerIndex: 1,
      explanation: "Network graphs use nodes (representing entities) and edges (representing relationships) to map out connections and flow, often arranged using physics-based layouts.",
    },
    {
      text: "A **Treemap** is a visualization technique used to display:",
      options: ["Hierarchical data using nested rectangles", "Geographical data", "Time series data", "Scatter plots for large datasets"],
      correctAnswerIndex: 0,
      explanation: "Treemaps are designed to show the structure of a hierarchy and the quantities for each category via the area of the rectangles.",
    },
    {
      text: "Which visualization technique is most effective for comparing the distributions of a continuous variable across multiple categories?",
      options: ["Pie Chart", "Box Plot (Box-and-Whisker Plot)", "Area Chart", "Bubble Chart"],
      correctAnswerIndex: 1,
      explanation: "Box plots efficiently display the five-number summary (minimum, Q1, median, Q3, maximum) for a distribution, making side-by-side comparisons of spread and central tendency easy.",
    },
    {
      text: "Which visualization is useful for exploring the correlation matrix of multiple variables?",
      options: ["Pie Chart", "Histogram", "Heatmap", "Streamgraph"],
      correctAnswerIndex: 2,
      explanation: "A heatmap uses a color-coded matrix where the color intensity of each cell represents the strength of the correlation between two variables.",
    },
    {
      text: "A **Word Cloud** is a visualization technique often used in Text Analysis to represent the:",
      options: ["Distribution of numerical values", "Frequency of words in a body of text, using size/color as encoding", "Relationships between two variables", "Time series trends"],
      correctAnswerIndex: 1,
      explanation: "Word clouds are simple visualizations where the importance or frequency of a word is shown by its size and, sometimes, its color.",
    },
    {
      text: "Which visualization technique can replace multiple box plots and shows the probability density of the data at different values?",
      options: ["Scatter Plot Matrix", "Violin Plot", "Sunburst Chart", "Parallel Coordinates Plot"],
      correctAnswerIndex: 1,
      explanation: "Violin plots display the probability density of the data at different values (like a smoothed histogram) and are often combined with a box plot inside to show the central statistics.",
    },
    {
      text: "Which visualization is used to display flow or movement of data between different states or regions?",
      options: ["Sunburst Chart", "Chord Diagram/Sankey Diagram", "Violin Plot", "Scatter Plot"],
      correctAnswerIndex: 1,
      explanation: "Sankey diagrams and Chord diagrams use flow lines (or chords) whose thickness is proportional to the flow quantity (e.g., energy, money, or movement).",
    },
    {
      text: "The main disadvantage of 3D visualizations is:",
      options: ["They are difficult to create", "They can distort perception and make interpretation harder", "They require special software", "They cannot handle large datasets"],
      correctAnswerIndex: 1,
      explanation: "3D charts are prone to occlusion (one element hiding another) and perspective distortion, which makes accurate comparison of values difficult.",
    },
    {
      text: "A **Geospatial Visualization** is a technique used to display data that is tied to:",
      options: ["Time", "Categories", "Geographical locations", "Numerical ranges"],
      correctAnswerIndex: 2,
      explanation: "Geospatial visualizations, such as maps, use location data (latitude and longitude) to plot data points or regions.",
    },
    {
      text: "What type of chart is often used to show changes in proportions over time, where the areas represent the magnitude of the values?",
      options: ["Scatter Plot", "Box Plot", "Stacked Area Chart", "Histogram"],
      correctAnswerIndex: 2,
      explanation: "Stacked area charts show the cumulative trend over time, where the height of the entire stack is the total, and the bands show the proportions of the parts that make up the total.",
    },
    {
      text: "When visualizing data, the concept of **Visual Hierarchy** helps the user by:",
      options: ["Ensuring all data is visible", "Guiding the eye to the most important elements first", "Making the chart colorful", "Encoding data redundantly"],
      correctAnswerIndex: 1,
      explanation: "Visual hierarchy uses contrast, size, and position to make certain elements stand out, directing the viewer's attention to the most critical information in a defined order.",
    },
    {
      text: "What is the standard number of variables displayed in a Line Chart?",
      options: ["One dependent variable and one independent variable", "At least three variables", "Two categorical variables", "Time/Sequence on one axis, and one or more numerical variables on the other"],
      correctAnswerIndex: 3,
      explanation: "The primary use of a line chart is to show how a numerical value changes over a continuous dimension, which is almost always time or a sequence (e.g., trial number).",
    },
    {
      text: "**Box plots** are particularly useful for:",
      options: ["Showing exact values", "Comparing distributions and identifying outliers", "Displaying proportions", "Showing trends over time"],
      correctAnswerIndex: 1,
      explanation: "The box and whiskers clearly show the interquartile range and the extent of the spread, with individual points outside the whiskers often highlighted as outliers.",
    },
    {
      text: "Which visualization is typically used to represent hierarchical data with a central root node and branching connections?",
      options: ["Heatmap", "Line Chart", "Dendrogram/Sunburst Chart", "Bar Chart"],
      correctAnswerIndex: 2,
      explanation: "Both dendrograms (tree diagrams) and Sunburst charts (circular treemaps) are specialized visualizations for showing parent-child relationships in a hierarchical structure.",
    },
    // Unit 5: Applications, Tools and Trends
    // Section A: Technologies and Architecture
    {
      text: "Bokeh is a popular interactive visualization library specifically developed for which programming language?",
      options: ["R", "Java", "Python", "MATLAB"],
      correctAnswerIndex: 2,
      explanation: "Bokeh is a Python library that enables the creation of elegant and interactive visualizations for modern web browsers.",
    },
    {
      text: "A key feature of Bokeh is its ability to create visualizations that are easily deployable in:",
      options: ["Microsoft Word documents", "Web browsers (using JavaScript/HTML)", "Command-line interfaces", "Desktop applications only"],
      correctAnswerIndex: 1,
      explanation: "Bokeh's core design goal is to provide interactive graphics that can be easily embedded into web pages or used in web-based dashboards.",
    },
    {
      text: "Which Python visualization library is often used for high-level statistical graphics based on Matplotlib?",
      options: ["Pandas", "NumPy", "Seaborn", "Scikit-learn"],
      correctAnswerIndex: 2,
      explanation: "Seaborn is a Python data visualization library based on Matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.",
    },
    {
      text: "What does **ETL** stand for in the context of data warehousing and data analysis?",
      options: ["Embedded Testing and Learning", "Extract, Transform, Load", "Error Threshold Limit", "Expert Training Logic"],
      correctAnswerIndex: 1,
      explanation: "ETL is a three-step process in data warehousing: **E**xtract data from source systems, **T**ransform it to fit business needs, and **L**oad it into the target system (like a data warehouse).",
    },
    {
      text: "The 'NoSQL' database trend is a response to the need to handle massive volumes of:",
      options: ["Structured data", "Unstructured and semi-structured data (Variety)", "Slowly changing data", "Only numerical data"],
      correctAnswerIndex: 1,
      explanation: 'NoSQL databases (like MongoDB, Cassandra) are designed to handle the "Variety" dimension of Big Data, accommodating the non-relational, flexible schemas of unstructured and semi-structured data.',
    },
    {
      text: "A **Dashboard** is a common application development method in data science for:",
      options: ["Training machine learning models", "Displaying key performance indicators (KPIs) and data visualizations in one place", "Writing data cleaning scripts", "Conducting statistical tests"],
      correctAnswerIndex: 1,
      explanation: "Dashboards provide a centralized, interactive view of data, using visualizations and metrics to help monitor performance and make data-driven decisions.",
    },
    {
      text: 'The fundamental concept of "Big Data" refers to datasets characterized by the three Vs: Volume, Velocity, and:',
      options: ["Validation", "Variety", "Value", "Visualization"],
      correctAnswerIndex: 1,
      explanation: "The three foundational characteristics of Big Data are **Volume** (sheer quantity of data), **Velocity** (speed of data creation and flow), and **Variety** (different forms of data).",
    },
    {
      text: "What is the fundamental difference between **Bokeh** and **Matplotlib**?",
      options: ["Matplotlib is for R, Bokeh is for Python.", "Bokeh is designed for interactive, web-browser-based visualization, while Matplotlib is historically static.", "Matplotlib can only do 2D plots.", "Bokeh is only for numerical data."],
      correctAnswerIndex: 1,
      explanation: "While Matplotlib has interactive backends, its traditional strength is creating static, publication-quality plots, whereas Bokeh is built from the ground up to render highly interactive plots for web applications.",
    },
    {
      text: "Which Python library provides the fundamental array and matrix operations necessary for data manipulation in most data science applications?",
      options: ["Matplotlib", "NumPy", "Scikit-learn", "Seaborn"],
      correctAnswerIndex: 1,
      explanation: "NumPy (Numerical Python) is the foundation of the scientific computing stack in Python, providing powerful N-dimensional array objects and mathematical tools.",
    },
    {
      text: "**D3.js** is a powerful JavaScript library often used for creating highly customized web-based visualizations. It primarily focuses on:",
      options: ["Statistical modeling", "Data-Driven Documents", "Server-side processing", "Mobile app development"],
      correctAnswerIndex: 1,
      explanation: "D3 stands for Data-Driven Documents. It uses HTML, SVG, and CSS to bring data to life in the browser, offering granular control over every visual element.",
    },
    // Section B: Emerging Trends and Concepts
    {
      text: "The technique of gathering data from various online sources, typically websites, is known as:",
      options: ["Data Farming", "Web Scraping/Crawling", "Data Fusion", "Data Imputation"],
      correctAnswerIndex: 1,
      explanation: 'Web scraping involves extracting data from websites, often using automated bots or scripts to "crawl" the web and collect content.',
    },
    {
      text: "In recent trends, the use of **Federated Learning** addresses concerns primarily related to:",
      options: ["Model accuracy", "Feature engineering", "Data privacy and security", "Model deployment speed"],
      correctAnswerIndex: 2,
      explanation: "Federated learning is a distributed machine learning approach that trains models on decentralized data (e.g., on mobile devices) without the data ever leaving the local device, thereby enhancing user privacy.",
    },
    {
      text: "The concept of **Model Interpretability** (Explainable AI - XAI) in applications is a growing trend due to:",
      options: ["High computational cost", "Ethical and regulatory requirements", "Need for more complex models", "Simplification of data collection"],
      correctAnswerIndex: 1,
      explanation: 'As AI models are used in critical domains (e.g., healthcare, finance), XAI is necessary to meet regulatory standards (like GDPR\\\'s "right to explanation") and ensure fairness and accountability.',
    },
    {
      text: "When developing a predictive application, what is the importance of **latency**?",
      options: ["It measures the complexity of the data", "It is the time delay between a request and a prediction response", "It is the volume of data", "It is the type of algorithm used"],
      correctAnswerIndex: 1,
      explanation: "Latency measures the time taken for a system to process a request and provide a result (a prediction). Low latency is critical for real-time applications.",
    },
    {
      text: "Which recent trend involves machines writing computer code or creating models with minimal human intervention?",
      options: ["Automated Data Cleaning", "Auto Machine Learning (AutoML)", "Manual Feature Engineering", "Standard Deviation"],
      correctAnswerIndex: 1,
      explanation: "AutoML aims to automate the end-to-end process of applying machine learning, including tasks like feature engineering, model selection, and hyperparameter tuning, thereby requiring less human expertise.",
    },
    {
      text: "The **Cloud Computing** paradigm (e.g., AWS, GCP, Azure) is a major trend supporting data science applications by providing:",
      options: ["Local storage only", "Scalable, on-demand compute and storage resources", "Only visualization tools", "Only open-source software"],
      correctAnswerIndex: 1,
      explanation: "Cloud platforms offer the elastic, pay-as-you-go infrastructure necessary to handle Big Data workloads and train large models without major upfront hardware investment.",
    },
    {
      text: "The application development method focused on continuous integration and continuous delivery (CI/CD) is a characteristic of:",
      options: ["Waterfall", "Six Sigma", "DevOps", "Lean Manufacturing"],
      correctAnswerIndex: 2,
      explanation: "DevOps is a set of practices that combines software development (Dev) and IT operations (Ops) to shorten the systems development life cycle and provide continuous delivery with high software quality, often through CI/CD pipelines.",
    },
    {
      text: "The method of application development where user feedback drives continuous, small iterations is the core of:",
      options: ["Waterfall", "Big Bang Approach", "Agile", "Monolithic"],
      correctAnswerIndex: 2,
      explanation: "Agile methodologies prioritize continuous feedback loops, adaptive planning, and quick, iterative development cycles over upfront, comprehensive planning.",
    },
    {
      text: "A recent trend in data collection and analysis involves using **Transfer Learning**, which is:",
      options: ["Moving data between databases", "Applying a model trained for one task to a different, but related, task", "Encrypting data during transfer", "Transferring features to a new model"],
      correctAnswerIndex: 1,
      explanation: "Transfer learning leverages knowledge gained from solving one problem (e.g., training an image recognition model) and applying it to a different but related problem (e.g., training a medical image classification model), which speeds up training and reduces data requirements.",
    },
    {
      text: "**A/B Testing** is an application method for:",
      options: ["Comparing two versions of a product to determine which performs better", "Training two machine learning models simultaneously", "Clustering data into two groups", "Visualizing two features on a plot"],
      correctAnswerIndex: 0,
      explanation: "A/B testing (or split testing) is a randomized experiment with two variants, A and B, used to test which version performs better against a defined conversion goal.",
    },
  ],
};